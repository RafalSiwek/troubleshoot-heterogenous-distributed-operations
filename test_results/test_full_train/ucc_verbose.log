Warning: Permanently added '[<rocm-ip>]:2222' (ED25519) to the list of known hosts.
Warning: Permanently added '[<cuda-ip>]:2222' (ED25519) to the list of known hosts.
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:1497: UserWarning: For MPI backend, world_size (2) and rank (1) are ignored since they are assigned by the MPI runtime.
  warnings.warn(
/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:1497: UserWarning: For MPI backend, world_size (2) and rank (0) are ignored since they are assigned by the MPI runtime.
  warnings.warn(
[1727983673.641184] [ip-cuda:1801 :0]   ucc_proc_info.c:309  UCC  DEBUG proc pid 1801, host ip-cuda, host_hash 3722453295186576972, sockid 0, numaid 0
[1727983673.641211] [ip-cuda:1801 :0] ucc_constructor.c:188  UCC  INFO  version: 1.4.0, loaded from: /usr/lib/libucc.so.1, cfg file: n/a
[1727983673.641237] [ip-cuda:1801 :0]          ucc_mc.c:67   UCC  DEBUG mc cpu mc initialized
[1727983673.641255] [ip-cuda:1801 :0]         mc_cuda.c:65   cuda mc DEBUG driver version 12040
[1727983673.641271] [ip-cuda:1801 :0]         mc_cuda.c:77   cuda mc DEBUG cuCtxGetDevice() failed: invalid device context
[1727983673.641280] [ip-cuda:1801 :0]          ucc_mc.c:67   UCC  DEBUG mc cuda mc initialized
[1727983673.641297] [ip-cuda:1801 :0]          ucc_ec.c:63   UCC  DEBUG ec cpu ec initialized
[1727983673.644479] [ip-rocm:1383 :0]   ucc_proc_info.c:309  UCC  DEBUG proc pid 1383, host ip-rocm, host_hash 6044294653273644824, sockid 0, numaid 0
[1727983673.644510] [ip-rocm:1383 :0] ucc_constructor.c:188  UCC  INFO  version: 1.4.0, loaded from: /usr/lib/libucc.so.1, cfg file: n/a
[1727983673.644525] [ip-rocm:1383 :0]          ucc_mc.c:67   UCC  DEBUG mc cpu mc initialized
[1727983673.644547] [ip-rocm:1383 :0]          ucc_mc.c:67   UCC  DEBUG mc rocm mc initialized
[1727983673.644567] [ip-rocm:1383 :0]          ucc_ec.c:63   UCC  DEBUG ec cpu ec initialized
[1727983673.644591] [ip-rocm:1383 :0]          ucc_ec.c:63   UCC  DEBUG ec rocm ec initialized
[1727983673.644635] [ip-rocm:1383 :0]    cl_basic_lib.c:20   CL_BASIC DEBUG initialized lib object: 0x91dc0c0
[1727983673.644656] [ip-rocm:1383 :0]         ucc_lib.c:150  UCC  DEBUG lib_prefix "OMPI_UCC_": initialized component "basic" score 10
[1727983673.644690] [ip-rocm:1383 :0]     tl_rccl_lib.c:18   TL_RCCL DEBUG initialized lib object: 0x91dc240
[1727983673.644718] [ip-rocm:1383 :0]     tl_self_lib.c:20   TL_SELF DEBUG initialized lib object: 0x91d1f00
[1727983673.644771] [ip-rocm:1383 :0]      tl_ucp_lib.c:69   TL_UCP DEBUG initialized lib object: 0x8abd840
[1727983673.644818] [ip-rocm:1383 :0] tl_rccl_context.c:82   TL_RCCL DEBUG fallback to event completion sync
[1727983673.644832] [ip-rocm:1383 :0] tl_rccl_context.c:87   TL_RCCL DEBUG using event completion sync
[1727983673.645452] [ip-rocm:1383 :0] tl_rccl_context.c:102  TL_RCCL DEBUG initialized tl context: 0x91df1c0
[1727983673.646784] [ip-cuda:1801 :0]          ucc_ec.c:63   UCC  DEBUG ec cuda ec initialized
[1727983673.646823] [ip-cuda:1801 :0]    cl_basic_lib.c:20   CL_BASIC DEBUG initialized lib object: 0x71a9a80
[1727983673.646839] [ip-cuda:1801 :0]         ucc_lib.c:150  UCC  DEBUG lib_prefix "OMPI_UCC_": initialized component "basic" score 10
[1727983673.646884] [ip-cuda:1801 :0]     tl_cuda_lib.c:35   TL_CUDA DEBUG initialized lib object: 0x71a9c90
[1727983673.646906] [ip-cuda:1801 :0]     tl_nccl_lib.c:16   TL_NCCL DEBUG initialized lib object: 0x71ac590
[1727983673.646926] [ip-cuda:1801 :0]     tl_self_lib.c:20   TL_SELF DEBUG initialized lib object: 0x71ac5f0
[1727983673.646961] [ip-cuda:1801 :0]      tl_ucp_lib.c:69   TL_UCP DEBUG initialized lib object: 0x6d533a0
[1727983673.647008] [ip-cuda:1801 :0] tl_cuda_context.c:42   TL_CUDA DEBUG cannot create CUDA TL context without active CUDA context
[1727983673.647020] [ip-cuda:1801 :0]     ucc_context.c:411  UCC  DEBUG failed to create tl context for cuda
[1727983673.647031] [ip-cuda:1801 :0] tl_nccl_context.c:182  TL_NCCL DEBUG using memops completion sync
[1727983673.648955] [ip-rocm:1383 :0]  tl_ucp_context.c:281  TL_UCP DEBUG initialized tl context: 0x91df710
[1727983673.648987] [ip-rocm:1383 :0] cl_basic_context.c:50   CL_BASIC DEBUG initialized cl context: 0x90ff720
[1727983673.849948] [ip-cuda:1801 :0] tl_nccl_context.c:205  TL_NCCL DEBUG initialized tl context: 0x71ac850
[1727983673.852747] [ip-cuda:1801 :0]  tl_ucp_context.c:281  TL_UCP DEBUG initialized tl context: 0x732e0e0
[1727983673.852773] [ip-cuda:1801 :0] cl_basic_context.c:38   CL_BASIC DEBUG TL cuda context is not available, skipping
[1727983673.852780] [ip-cuda:1801 :0] cl_basic_context.c:50   CL_BASIC DEBUG initialized cl context: 0x8037410
[1727983673.860567] [ip-cuda:1801 :0]     tl_ucp_team.c:103  TL_UCP DEBUG posted tl team: 0x80c11a0
[1727983673.860580] [ip-cuda:1801 :0]     tl_ucp_team.c:202  TL_UCP DEBUG initialized tl team: 0x80c11a0
[1727983673.860594] [ip-cuda:1801 :0]     ucc_context.c:839  UCC  DEBUG created ucc context 0x71ac6e0 for lib OMPI_UCC_
[1727983673.860633] [ip-cuda:1801 :0]        ucc_team.c:369  UCC  DEBUG team 0x6dcd950 rank 1, ctx_rank 1, map_type 1
[1727983673.860653] [ip-cuda:1801 :0]          ucc_tl.c:227  UCC  DEBUG TL nccl is not reachable, skipping
[1727983673.860660] [ip-cuda:1801 :0]   cl_basic_team.c:52   CL_BASIC DEBUG posted cl team: 0x1bde970
[1727983673.860674] [ip-cuda:1801 :0]          ucc_tl.c:298  TL_SELF DEBUG team size 2 is too big, max supported 1
[1727983673.860695] [ip-cuda:1801 :0]     tl_ucp_team.c:100  TL_UCP DEBUG opt knomial radix: 2
[1727983673.860701] [ip-cuda:1801 :0]     tl_ucp_team.c:103  TL_UCP DEBUG posted tl team: 0x80c2f00
[1727983673.860709] [ip-cuda:1801 :0]     tl_ucp_team.c:202  TL_UCP DEBUG initialized tl team: 0x80c2f00
[1727983673.860718] [ip-cuda:1801 :0]   cl_basic_team.c:124  CL_BASIC DEBUG failed to create tl nccl team: (-7)
[1727983673.860713] [ip-rocm:1383 :0]     tl_ucp_team.c:103  TL_UCP DEBUG posted tl team: 0x92a4600
[1727983673.860743] [ip-rocm:1383 :0]     tl_ucp_team.c:202  TL_UCP DEBUG initialized tl team: 0x92a4600
[1727983673.860748] [ip-rocm:1383 :0]     ucc_context.c:839  UCC  DEBUG created ucc context 0x91df050 for lib OMPI_UCC_
[1727983673.860725] [ip-cuda:1801 :0]   cl_basic_team.c:124  CL_BASIC DEBUG failed to create tl self team: (-1)
[1727983673.860733] [ip-cuda:1801 :0]   cl_basic_team.c:120  CL_BASIC DEBUG initialized tl ucp team
[1727983673.860743] [ip-cuda:1801 :0]     tl_ucp_team.c:230  TL_UCP DEBUG enable support for memory type host
[1727983673.860777] [ip-rocm:1383 :0]        ucc_team.c:369  UCC  DEBUG team 0x92a6450 rank 0, ctx_rank 0, map_type 1
[1727983673.860798] [ip-rocm:1383 :0]          ucc_tl.c:227  UCC  DEBUG TL rccl is not reachable, skipping
[1727983673.860812] [ip-rocm:1383 :0]   cl_basic_team.c:52   CL_BASIC DEBUG posted cl team: 0x92a68c0
[1727983673.860822] [ip-rocm:1383 :0]          ucc_tl.c:298  TL_SELF DEBUG team size 2 is too big, max supported 1
[1727983673.860858] [ip-rocm:1383 :0]     tl_ucp_team.c:100  TL_UCP DEBUG opt knomial radix: 2
[1727983673.860871] [ip-rocm:1383 :0]     tl_ucp_team.c:103  TL_UCP DEBUG posted tl team: 0x92a6a10
[1727983673.860877] [ip-rocm:1383 :0]     tl_ucp_team.c:202  TL_UCP DEBUG initialized tl team: 0x92a6a10
[1727983673.860882] [ip-rocm:1383 :0]   cl_basic_team.c:124  CL_BASIC DEBUG failed to create tl rccl team: (-7)
[1727983673.860886] [ip-rocm:1383 :0]   cl_basic_team.c:124  CL_BASIC DEBUG failed to create tl self team: (-1)
[1727983673.860900] [ip-rocm:1383 :0]   cl_basic_team.c:120  CL_BASIC DEBUG initialized tl ucp team
[1727983673.860907] [ip-rocm:1383 :0]     tl_ucp_team.c:230  TL_UCP DEBUG enable support for memory type host
[1727983673.861049] [ip-rocm:1383 :0]        ucc_team.c:471  UCC  INFO  ===== COLL_SCORE_MAP (team_id 32768, size 2) =====
[1727983673.861080] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Allgather:
[1727983673.861080] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..4095}:TL_UCP:10 {4K..inf}:TL_UCP:10 
[1727983673.861111] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Allgatherv:
[1727983673.861111] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861132] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Allreduce:
[1727983673.861132] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..4095}:TL_UCP:10 {4K..inf}:TL_UCP:10 
[1727983673.861152] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Alltoall:
[1727983673.861152] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..257}:TL_UCP:10 {258..inf}:TL_UCP:10 
[1727983673.861189] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Alltoallv:
[1727983673.861189] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861199] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Barrier:
[1727983673.861199] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861218] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Bcast:
[1727983673.861218] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861236] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Fanin:
[1727983673.861236] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861255] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Fanout:
[1727983673.861255] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861273] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Gather:
[1727983673.861273] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861290] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Gatherv:
[1727983673.861290] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861300] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Reduce:
[1727983673.861300] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861319] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Reduce_scatter:
[1727983673.861319] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861338] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Reduce_scatterv:
[1727983673.861338] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861348] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  Scatterv:
[1727983673.861348] [ip-rocm:1383 :0] ucc_coll_score_map.c:203  UCC  INFO  	Host: {0..inf}:TL_UCP:10 
[1727983673.861356] [ip-rocm:1383 :0]        ucc_team.c:474  UCC  INFO  ================================================
[1727983674.700598] [ip-cuda:1801 :1] ucc_coll_score_map.c:123  UCC  DEBUG coll_score_map lookup failed -1 (Operation is not supported)
[1727983674.700620] [ip-cuda:1801 :1]        ucc_coll.c:247  UCC  DEBUG failed to init collective: not supported
[ip-cuda:1801 :1:1807]  ucp_worker.c:3027 Assertion `ucs_async_check_owner_thread(&(worker)->async)' failed
==== backtrace (tid:   1807) ====
 0  /usr/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x704386728aac]
 1  /usr/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7043867258d8]
 2  /usr/lib/libucs.so.0(ucs_fatal_error_format+0x11d) [0x7043867259fd]
 3  /usr/lib/libucp.so.0(ucp_worker_progress+0x218) [0x7043862abd78]
 4  /usr/lib/libucc.so.1(ucc_context_progress+0x7f) [0x704386787d7f]
 5  /usr/lib/libmpi.so.40(+0x1341cb) [0x70438d4c01cb]
 6  /usr/lib/libopen-pal.so.80(opal_progress+0x34) [0x70438614d744]
 7  /usr/lib/libmpi.so.40(ompi_request_default_wait+0x143) [0x70438d422613]
 8  /usr/lib/libmpi.so.40(ompi_coll_base_sendrecv_actual+0xe5) [0x70438d495485]
 9  /usr/lib/libmpi.so.40(ompi_coll_base_allreduce_intra_recursivedoubling+0x2ae) [0x70438d4967ae]
10  /usr/lib/libmpi.so.40(ompi_coll_base_allreduce_intra_ring+0x889) [0x70438d499309]
11  /usr/lib/libmpi.so.40(ompi_coll_tuned_allreduce_intra_dec_fixed+0x49) [0x70438d4e73b9]
12  /usr/lib/libmpi.so.40(mca_coll_han_comm_create_new+0x293) [0x70438d51d163]
13  /usr/lib/libmpi.so.40(mca_coll_han_allgather_intra+0x55) [0x70438d514925]
14  /usr/lib/libmpi.so.40(mca_coll_ucc_allgather+0xc1) [0x70438d4c3ef1]
15  /usr/lib/libmpi.so.40(PMPI_Allgather+0x11e) [0x70438d437e6e]
16  /opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so(+0x6136983) [0x70437e0b9983]
17  /opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so(_ZN4c10d15ProcessGroupMPI7runLoopEv+0x112) [0x70437e0bc8e2]
18  /opt/conda/envs/py_3.12/lib/libstdc++.so.6(+0xcdaeb) [0x70435f719aeb]
19  /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x70438e040ac3]
20  /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x70438e0d2850]
=================================
[ip-cuda:01801] *** Process received signal ***
[ip-cuda:01801] Signal: Aborted (6)
[ip-cuda:01801] Signal code:  (-6)
[ip-cuda:01801] [ 0] /usr/lib/x86_64-linux-gnu/libc.so.6(+0x42520)[0x70438dfee520]
[ip-cuda:01801] [ 1] /usr/lib/x86_64-linux-gnu/libc.so.6(pthread_kill+0x12c)[0x70438e0429fc]
[ip-cuda:01801] [ 2] /usr/lib/x86_64-linux-gnu/libc.so.6(raise+0x16)[0x70438dfee476]
[ip-cuda:01801] [ 3] /usr/lib/x86_64-linux-gnu/libc.so.6(abort+0xd3)[0x70438dfd47f3]
[ip-cuda:01801] [ 4] /usr/lib/libucs.so.0(+0x398dd)[0x7043867258dd]
[ip-cuda:01801] [ 5] /usr/lib/libucs.so.0(ucs_fatal_error_format+0x11d)[0x7043867259fd]
[ip-cuda:01801] [ 6] /usr/lib/libucp.so.0(ucp_worker_progress+0x218)[0x7043862abd78]
[ip-cuda:01801] [ 7] /usr/lib/libucc.so.1(ucc_context_progress+0x7f)[0x704386787d7f]
[ip-cuda:01801] [ 8] /usr/lib/libmpi.so.40(+0x1341cb)[0x70438d4c01cb]
[ip-cuda:01801] [ 9] /usr/lib/libopen-pal.so.80(opal_progress+0x34)[0x70438614d744]
[ip-cuda:01801] [10] /usr/lib/libmpi.so.40(ompi_request_default_wait+0x143)[0x70438d422613]
[ip-cuda:01801] [11] /usr/lib/libmpi.so.40(ompi_coll_base_sendrecv_actual+0xe5)[0x70438d495485]
[ip-cuda:01801] [12] /usr/lib/libmpi.so.40(ompi_coll_base_allreduce_intra_recursivedoubling+0x2ae)[0x70438d4967ae]
[ip-cuda:01801] [13] /usr/lib/libmpi.so.40(ompi_coll_base_allreduce_intra_ring+0x889)[0x70438d499309]
[ip-cuda:01801] [14] /usr/lib/libmpi.so.40(ompi_coll_tuned_allreduce_intra_dec_fixed+0x49)[0x70438d4e73b9]
[ip-cuda:01801] [15] /usr/lib/libmpi.so.40(mca_coll_han_comm_create_new+0x293)[0x70438d51d163]
[ip-cuda:01801] [16] /usr/lib/libmpi.so.40(mca_coll_han_allgather_intra+0x55)[0x70438d514925]
[ip-cuda:01801] [17] /usr/lib/libmpi.so.40(mca_coll_ucc_allgather+0xc1)[0x70438d4c3ef1]
[ip-cuda:01801] [18] /usr/lib/libmpi.so.40(PMPI_Allgather+0x11e)[0x70438d437e6e]
[ip-cuda:01801] [19] /opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so(+0x6136983)[0x70437e0b9983]
[ip-cuda:01801] [20] /opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so(_ZN4c10d15ProcessGroupMPI7runLoopEv+0x112)[0x70437e0bc8e2]
[ip-cuda:01801] [21] /opt/conda/envs/py_3.12/lib/libstdc++.so.6(+0xcdaeb)[0x70435f719aeb]
[ip-cuda:01801] [22] /usr/lib/x86_64-linux-gnu/libc.so.6(+0x94ac3)[0x70438e040ac3]
[ip-cuda:01801] [23] /usr/lib/x86_64-linux-gnu/libc.so.6(+0x126850)[0x70438e0d2850]
[ip-cuda:01801] *** End of error message ***
[1727983674.923527] [ip-rocm:1383 :1] ucc_coll_score_map.c:123  UCC  DEBUG coll_score_map lookup failed -1 (Operation is not supported)
[1727983674.923561] [ip-rocm:1383 :1]        ucc_coll.c:247  UCC  DEBUG failed to init collective: not supported
[ip-rocm:1383 :1:1390] Caught signal 11 (Segmentation fault: invalid permissions for mapped object at address 0x7a9304a00400)
==== backtrace (tid:   1390) ====
 0  /usr/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7a94d74acaac]
 1  /usr/lib/libucs.so.0(+0x3cc8f) [0x7a94d74acc8f]
 2  /usr/lib/libucs.so.0(+0x3cfc4) [0x7a94d74acfc4]
 3  /lib/x86_64-linux-gnu/libc.so.6(+0x42520) [0x7a94def35520]
 4  /lib/x86_64-linux-gnu/libc.so.6(+0x1a0840) [0x7a94df093840]
 5  /usr/lib/libuct.so.0(uct_tcp_ep_am_short+0xf3) [0x7a94d704fcd3]
 6  /usr/lib/libucp.so.0(+0x10cf16) [0x7a94d7609f16]
 7  /usr/lib/libucp.so.0(ucp_tag_send_nbx+0x1780) [0x7a94d761a180]
 8  /usr/lib/libmpi.so.40(mca_pml_ucx_send+0x147) [0x7a94de553e07]
 9  /usr/lib/libmpi.so.40(ompi_coll_base_sendrecv_actual+0xc9) [0x7a94de3e0469]
10  /usr/lib/libmpi.so.40(ompi_coll_base_allgather_intra_two_procs+0x88) [0x7a94de3deee8]
11  /usr/lib/libmpi.so.40(ompi_coll_tuned_allgather_intra_dec_fixed+0x4e) [0x7a94de431c7e]
12  /usr/lib/libmpi.so.40(mca_coll_han_allgather_intra+0xea) [0x7a94de45e06a]
13  /usr/lib/libmpi.so.40(mca_coll_ucc_allgather+0xc1) [0x7a94de40eef1]
14  /usr/lib/libmpi.so.40(PMPI_Allgather+0x11e) [0x7a94de382e6e]
15  /opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so(+0x5f03943) [0x7a94d147d943]
16  /opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so(_ZN4c10d15ProcessGroupMPI7runLoopEv+0x112) [0x7a94d14808a2]
17  /opt/conda/envs/py_3.12/lib/libstdc++.so.6(+0xcdaeb) [0x7a949295daeb]
18  /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7a94def87ac3]
19  /lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7a94df019850]
=================================
--------------------------------------------------------------------------
    This help section is empty because PRRTE was built without Sphinx.
--------------------------------------------------------------------------
